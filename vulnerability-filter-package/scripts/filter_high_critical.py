#!/usr/bin/env python3
"""
Filter High and Critical Vulnerabilities
Creates a clean JSON file containing only high/critical severity vulnerabilities
"""

import json
import argparse
import logging
import os
import sys
import datetime
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def get_severity_level(vuln):
    """Determine the severity level of a vulnerability."""
    # Check Aqua severity first
    aqua_severity = vuln.get('aqua_severity', '').lower()
    if aqua_severity in ['critical', 'high']:
        return aqua_severity
    
    # Check NVD severity
    nvd_severity = vuln.get('nvd_severity_v3', '').lower()
    if nvd_severity in ['critical', 'high']:
        return nvd_severity
    
    # Check by CVSS score
    cvss_score = vuln.get('aqua_score', vuln.get('nvd_score_v3', 0))
    try:
        score = float(cvss_score)
        if score >= 9.0:
            return 'critical'
        elif score >= 7.0:
            return 'high'
    except (ValueError, TypeError):
        pass
    
    return None

def filter_high_critical(input_file, output_file):
    """
    Filter vulnerabilities to include only high and critical severity.
    
    Args:
        input_file (str): Path to the original Aqua scan JSON report
        output_file (str): Path where the filtered report will be saved
    """
    try:
        # Load scan data
        with open(input_file, 'r') as f:
            scan_data = json.load(f)
        
        logging.info(f"Loaded scan report from {input_file}")
        
        # Statistics
        stats = defaultdict(int)
        total_vulns = 0
        
        # Create filtered data structure
        filtered_data = {
            'scan_metadata': {
                'image': scan_data.get('image', ''),
                'registry': scan_data.get('registry', ''),
                'scan_started': scan_data.get('scan_started', {}),
                'digest': scan_data.get('digest', ''),
                'filter_applied': 'high_critical_only',
                'filter_timestamp': datetime.datetime.now().isoformat()
            },
            'vulnerabilities': [],
            'summary': {}
        }
        
        # Process resources
        for resource in scan_data.get('resources', []):
            resource_info = resource.get('resource', {})
            
            for vuln in resource.get('vulnerabilities', []):
                total_vulns += 1
                severity = get_severity_level(vuln)
                
                if severity in ['high', 'critical']:
                    # Create a clean vulnerability object
                    clean_vuln = {
                        'cve_id': vuln.get('name', 'Unknown'),
                        'severity': severity.upper(),
                        'cvss_score': vuln.get('aqua_score', vuln.get('nvd_score_v3', 0)),
                        'cvss_vector': vuln.get('aqua_vectors', vuln.get('nvd_vectors_v3', '')),
                        'resource': {
                            'name': resource_info.get('name', ''),
                            'version': resource_info.get('version', ''),
                            'type': resource_info.get('type', ''),
                            'path': resource_info.get('path', '')
                        },
                        'description': vuln.get('description', ''),
                        'fix_version': vuln.get('fix_version', ''),
                        'solution': vuln.get('solution', ''),
                        'publish_date': vuln.get('publish_date', ''),
                        'exploitability': {
                            'exploit_available': vuln.get('exploit_available', False),
                            'exploitability_score': vuln.get('exploitability_score', 0),
                            'has_exploit': vuln.get('has_exploit', False)
                        }
                    }
                    
                    filtered_data['vulnerabilities'].append(clean_vuln)
                    stats[severity] += 1
        
        # Process direct vulnerabilities if present
        for vuln in scan_data.get('vulnerabilities', []):
            total_vulns += 1
            severity = get_severity_level(vuln)
            
            if severity in ['high', 'critical']:
                clean_vuln = {
                    'cve_id': vuln.get('name', 'Unknown'),
                    'severity': severity.upper(),
                    'cvss_score': vuln.get('aqua_score', vuln.get('nvd_score_v3', 0)),
                    'cvss_vector': vuln.get('aqua_vectors', vuln.get('nvd_vectors_v3', '')),
                    'resource': {
                        'name': vuln.get('resource_name', ''),
                        'version': vuln.get('resource_version', ''),
                        'type': vuln.get('resource_type', ''),
                        'path': vuln.get('resource_path', '')
                    },
                    'description': vuln.get('description', ''),
                    'fix_version': vuln.get('fix_version', ''),
                    'solution': vuln.get('solution', ''),
                    'publish_date': vuln.get('publish_date', ''),
                    'exploitability': {
                        'exploit_available': vuln.get('exploit_available', False),
                        'exploitability_score': vuln.get('exploitability_score', 0),
                        'has_exploit': vuln.get('has_exploit', False)
                    }
                }
                
                filtered_data['vulnerabilities'].append(clean_vuln)
                stats[severity] += 1
        
        # Sort vulnerabilities by CVSS score (highest first)
        filtered_data['vulnerabilities'].sort(
            key=lambda x: x.get('cvss_score', 0), 
            reverse=True
        )
        
        # Add summary statistics
        filtered_data['summary'] = {
            'total_vulnerabilities_scanned': total_vulns,
            'high_critical_count': stats['high'] + stats['critical'],
            'critical_count': stats['critical'],
            'high_count': stats['high'],
            'reduction_percentage': round(
                (1 - (stats['high'] + stats['critical']) / max(1, total_vulns)) * 100, 2
            ),
            'risk_score': calculate_risk_score(stats),
            'recommendations': generate_recommendations(stats)
        }
        
        # Write filtered report
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(filtered_data, f, indent=2)
        
        # Create a companion CSV file for easy viewing
        csv_file = output_file.replace('.json', '.csv')
        create_csv_report(filtered_data['vulnerabilities'], csv_file)
        
        logging.info(f"Filtered report saved to {output_file}")
        logging.info(f"Found {stats['critical']} critical and {stats['high']} high vulnerabilities")
        logging.info(f"Reduced data by {filtered_data['summary']['reduction_percentage']}%")
        
        return True
        
    except Exception as e:
        logging.error(f"Error processing report: {str(e)}")
        return False

def calculate_risk_score(stats):
    """Calculate an overall risk score based on vulnerability distribution."""
    # Simple weighted scoring: Critical = 10, High = 5
    score = (stats['critical'] * 10) + (stats['high'] * 5)
    
    if score >= 50:
        return {'score': score, 'level': 'CRITICAL', 'action': 'IMMEDIATE ACTION REQUIRED'}
    elif score >= 20:
        return {'score': score, 'level': 'HIGH', 'action': 'URGENT REMEDIATION NEEDED'}
    elif score >= 10:
        return {'score': score, 'level': 'MEDIUM', 'action': 'PLAN REMEDIATION'}
    else:
        return {'score': score, 'level': 'LOW', 'action': 'MONITOR'}

def generate_recommendations(stats):
    """Generate actionable recommendations based on findings."""
    recommendations = []
    
    if stats['critical'] > 0:
        recommendations.append(f"CRITICAL: {stats['critical']} critical vulnerabilities require immediate patching")
    
    if stats['high'] > 5:
        recommendations.append(f"HIGH: {stats['high']} high vulnerabilities should be addressed within 30 days")
    
    if stats['critical'] + stats['high'] == 0:
        recommendations.append("GOOD: No high or critical vulnerabilities found")
    
    return recommendations

def create_csv_report(vulnerabilities, csv_file):
    """Create a CSV report for easy viewing in spreadsheets."""
    import csv
    
    with open(csv_file, 'w', newline='') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow([
            'CVE ID', 'Severity', 'CVSS Score', 'Resource', 'Version', 
            'Fix Version', 'Exploit Available', 'Description'
        ])
        
        # Data
        for vuln in vulnerabilities:
            writer.writerow([
                vuln['cve_id'],
                vuln['severity'],
                vuln['cvss_score'],
                vuln['resource']['name'],
                vuln['resource']['version'],
                vuln['fix_version'],
                'YES' if vuln['exploitability']['exploit_available'] else 'NO',
                vuln['description'][:100] + '...' if len(vuln['description']) > 100 else vuln['description']
            ])
    
    logging.info(f"CSV report saved to {csv_file}")

def main():
    parser = argparse.ArgumentParser(
        description='Filter Aqua scan results to show only high and critical vulnerabilities'
    )
    parser.add_argument('input_file', help='Path to the original Aqua scan JSON report')
    parser.add_argument('output_file', help='Path where the filtered report will be saved')
    
    args = parser.parse_args()
    
    success = filter_high_critical(args.input_file, args.output_file)
    
    # Exit with appropriate code for CI/CD integration
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()