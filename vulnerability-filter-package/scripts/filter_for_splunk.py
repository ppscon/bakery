#!/usr/bin/env python3
import json
import argparse
import logging
import os
import sys
import datetime
import hashlib
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def filter_for_splunk(input_file, output_file, min_severity=7.0, delta_only=False, policy_violations_only=False, previous_scan=None):
    """
    Filter Aqua scan results specifically for Splunk ingestion.
    
    Args:
        input_file (str): Path to the original Aqua scan JSON report
        output_file (str): Path where the filtered report will be saved
        min_severity (float): Minimum CVSS score threshold (default: 7.0)
        delta_only (bool): If True, only include newly discovered vulnerabilities
        policy_violations_only (bool): If True, only include vulnerabilities that violate policies
        previous_scan (str): Path to previous scan report for delta comparison
    """
    try:
        # Load current scan data
        with open(input_file, 'r') as f:
            scan_data = json.load(f)
        
        logging.info(f"Loaded scan report from {input_file}")
        
        # Load previous scan data if delta reporting is enabled
        previous_vulns = set()
        if delta_only and previous_scan and os.path.exists(previous_scan):
            with open(previous_scan, 'r') as f:
                prev_data = json.load(f)
            
            # Extract vulnerability IDs from previous scan
            for resource in prev_data.get('resources', []):
                for vuln in resource.get('vulnerabilities', []):
                    if 'name' in vuln:
                        # Create a unique identifier for the vulnerability
                        vuln_id = f"{vuln['name']}:{resource['resource'].get('name', '')}:{resource['resource'].get('version', '')}"
                        previous_vulns.add(vuln_id)
            
            # Also check direct vulnerabilities array
            for vuln in prev_data.get('vulnerabilities', []):
                if 'name' in vuln:
                    vuln_id = f"{vuln['name']}:{vuln.get('resource_name', '')}:{vuln.get('resource_version', '')}"
                    previous_vulns.add(vuln_id)
            
            logging.info(f"Loaded {len(previous_vulns)} vulnerabilities from previous scan for delta comparison")
        
        # Create filtered scan data structure
        filtered_data = {
            'image': scan_data.get('image', ''),
            'registry': scan_data.get('registry', ''),
            'scan_started': scan_data.get('scan_started', {}),
            'digest': scan_data.get('digest', ''),
            'os': scan_data.get('os', ''),
            'version': scan_data.get('version', ''),
            'resources': [],
            'metadata': {
                'original_size': len(json.dumps(scan_data)),
                'filtering_criteria': {
                    'min_severity': min_severity,
                    'delta_only': delta_only,
                    'policy_violations_only': policy_violations_only
                },
                'filter_timestamp': datetime.datetime.now().isoformat()
            }
        }
        
        # Statistics for the summary
        stats = {
            'total_vulnerabilities': 0,
            'filtered_vulnerabilities': 0,
            'included_by_severity': 0,
            'excluded_by_severity': 0,
            'excluded_by_delta': 0,
            'excluded_by_policy': 0,
            'resource_count': 0,
            'severity_counts': defaultdict(int)
        }
        
        # Process resources with vulnerabilities
        if 'resources' in scan_data:
            for resource in scan_data['resources']:
                if 'vulnerabilities' in resource:
                    stats['total_vulnerabilities'] += len(resource['vulnerabilities'])
                    
                    # Filter vulnerabilities for this resource
                    filtered_vulns = []
                    for vuln in resource['vulnerabilities']:
                        # Create unique vulnerability ID for delta comparison
                        vuln_id = f"{vuln.get('name', '')}:{resource['resource'].get('name', '')}:{resource['resource'].get('version', '')}"
                        
                        # Check severity - use highest available score
                        severity_score = max(
                            float(vuln.get('nvd_score_v3', 0)),
                            float(vuln.get('aqua_score', 0)),
                            float(vuln.get('vendor_score', 0))
                        )
                        
                        # Track stats by severity
                        severity_level = vuln.get('aqua_severity', 'unknown')
                        stats['severity_counts'][severity_level] += 1
                        
                        # Apply filters
                        include_vuln = True
                        
                        # Severity filter
                        if severity_score < min_severity:
                            include_vuln = False
                            stats['excluded_by_severity'] += 1
                        else:
                            stats['included_by_severity'] += 1
                        
                        # Delta filter (only include new vulnerabilities)
                        if include_vuln and delta_only and vuln_id in previous_vulns:
                            include_vuln = False
                            stats['excluded_by_delta'] += 1
                        
                        # Policy violations filter
                        if include_vuln and policy_violations_only:
                            is_policy_violation = (
                                vuln.get('is_violating_policy', False) or
                                vuln.get('violates_policy', False) or
                                any(label.lower().startswith('policy') for label in vuln.get('labels', []))
                            )
                            if not is_policy_violation:
                                include_vuln = False
                                stats['excluded_by_policy'] += 1
                        
                        # Include the vulnerability if it passes all filters
                        if include_vuln:
                            # Trim unnecessary fields to reduce payload size
                            trimmed_vuln = {
                                'name': vuln.get('name', ''),
                                'severity': vuln.get('aqua_severity', ''),
                                'score': severity_score,
                                'fix_version': vuln.get('fix_version', ''),
                                'resource': resource['resource'].get('name', ''),
                                'resource_version': resource['resource'].get('version', ''),
                                'resource_path': resource['resource'].get('path', ''),
                                'description': vuln.get('description', '')[:500],  # Truncate long descriptions
                                'solution': vuln.get('solution', '')
                            }
                            
                            # Add critical fields for high-severity issues
                            if severity_score >= 9.0:
                                trimmed_vuln['vectors'] = vuln.get('aqua_vectors', vuln.get('nvd_vectors_v3', ''))
                                trimmed_vuln['publish_date'] = vuln.get('publish_date', '')
                                trimmed_vuln['nvd_url'] = vuln.get('nvd_url', '')
                            
                            filtered_vulns.append(trimmed_vuln)
                            stats['filtered_vulnerabilities'] += 1
                    
                    # Only include resources that have vulnerabilities after filtering
                    if filtered_vulns:
                        filtered_resource = {
                            'resource': {
                                'name': resource['resource'].get('name', ''),
                                'version': resource['resource'].get('version', ''),
                                'format': resource['resource'].get('format', ''),
                                'path': resource['resource'].get('path', '')
                            },
                            'vulnerabilities': filtered_vulns
                        }
                        filtered_data['resources'].append(filtered_resource)
                        stats['resource_count'] += 1
        
        # Update metadata with filtering stats
        filtered_data['metadata']['filtering_stats'] = {
            'total_vulnerabilities': stats['total_vulnerabilities'],
            'included_vulnerabilities': stats['filtered_vulnerabilities'],
            'excluded_vulnerabilities': stats['total_vulnerabilities'] - stats['filtered_vulnerabilities'],
            'included_resources': stats['resource_count'],
            'severity_distribution': dict(stats['severity_counts']),
            'filter_reasons': {
                'below_severity_threshold': stats['excluded_by_severity'],
                'found_in_previous_scan': stats['excluded_by_delta'],
                'no_policy_violation': stats['excluded_by_policy']
            }
        }
        
        # Calculate size reduction percentage
        filtered_size = len(json.dumps(filtered_data))
        filtered_data['metadata']['size_reduction'] = {
            'original_bytes': filtered_data['metadata']['original_size'],
            'filtered_bytes': filtered_size,
            'reduction_percentage': round((1 - (filtered_size / filtered_data['metadata']['original_size'])) * 100, 2)
        }
        
        # Create individual vulnerability objects for direct Splunk ingestion
        splunk_events = []
        scan_id = hashlib.md5(f"{filtered_data['image']}:{filtered_data['digest']}:{filtered_data['scan_started']}".encode()).hexdigest()
        
        for resource in filtered_data['resources']:
            for vuln in resource['vulnerabilities']:
                # Create a separate event for each vulnerability
                event = {
                    'scan_id': scan_id,
                    'timestamp': datetime.datetime.now().isoformat(),
                    'image': filtered_data['image'],
                    'digest': filtered_data['digest'],
                    'os': filtered_data['os'],
                    'os_version': filtered_data['version'],
                    'resource_name': resource['resource']['name'],
                    'resource_version': resource['resource']['version'],
                    'resource_path': resource['resource']['path'],
                    'resource_format': resource['resource']['format'],
                    'vulnerability_id': vuln['name'],
                    'severity': vuln['severity'],
                    'score': vuln['score'],
                    'description': vuln['description'],
                    'solution': vuln['solution'],
                    'fix_version': vuln['fix_version'],
                    'event_type': 'aqua_security_vulnerability'
                }
                
                # Add optional fields if present
                for field in ['vectors', 'publish_date', 'nvd_url']:
                    if field in vuln:
                        event[field] = vuln[field]
                
                splunk_events.append(event)
        
        # Write the filtered report
        with open(output_file, 'w') as f:
            json.dump(filtered_data, f, indent=2)
        
        # Write Splunk-optimized events file
        splunk_output = output_file.replace('.json', '_splunk.json')
        with open(splunk_output, 'w') as f:
            # Write one event per line in JSON format (for Splunk HEC)
            for event in splunk_events:
                f.write(json.dumps(event) + '\n')
        
        # Create a summary file
        summary_file = output_file.replace('.json', '_summary.json')
        with open(summary_file, 'w') as f:
            summary = {
                'scan_info': {
                    'image': filtered_data['image'],
                    'scan_time': filtered_data['scan_started'],
                    'filter_time': filtered_data['metadata']['filter_timestamp']
                },
                'filtering_summary': {
                    'total_vulnerabilities': stats['total_vulnerabilities'],
                    'vulnerabilities_included': stats['filtered_vulnerabilities'],
                    'resources_with_vulnerabilities': stats['resource_count'],
                    'filtering_criteria': filtered_data['metadata']['filtering_criteria'],
                    'severity_distribution': dict(stats['severity_counts']),
                    'size_reduction': filtered_data['metadata']['size_reduction']
                }
            }
            json.dump(summary, f, indent=2)
        
        logging.info(f"Original report had {stats['total_vulnerabilities']} vulnerabilities")
        logging.info(f"Filtered report has {stats['filtered_vulnerabilities']} vulnerabilities")
        logging.info(f"Size reduction: {filtered_data['metadata']['size_reduction']['reduction_percentage']}%")
        logging.info(f"Filtered report saved to {output_file}")
        logging.info(f"Splunk-optimized events saved to {splunk_output}")
        logging.info(f"Summary saved to {summary_file}")
        
        return True, filtered_data['metadata']['size_reduction']['reduction_percentage']
    
    except Exception as e:
        logging.error(f"Error processing report: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        return False, 0

def main():
    parser = argparse.ArgumentParser(description='Filter Aqua scan results for Splunk ingestion')
    parser.add_argument('input_file', help='Path to the original Aqua scan JSON report')
    parser.add_argument('output_file', help='Path where the filtered report will be saved')
    parser.add_argument('--min-severity', type=float, default=7.0, help='Minimum CVSS score threshold (default: 7.0)')
    parser.add_argument('--delta-only', action='store_true', help='Only include newly discovered vulnerabilities')
    parser.add_argument('--policy-violations-only', action='store_true', help='Only include vulnerabilities that violate policies')
    parser.add_argument('--previous-scan', help='Path to previous scan report for delta comparison')
    
    args = parser.parse_args()
    
    success, reduction = filter_for_splunk(
        args.input_file, 
        args.output_file,
        args.min_severity,
        args.delta_only,
        args.policy_violations_only,
        args.previous_scan
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 